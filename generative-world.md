# üåç Generative World Models

Generative world models allow embodied AI agents to build internal representations of their environment and simulate possible futures before acting. 

They are a cornerstone of model-based reinforcement learning, safe planning, and adaptive robotics.



## üìÑ Key Papers

### Foundational World Models
- [World Models (Ha & Schmidhuber, 2018)](https://arxiv.org/abs/1803.10122) ‚Äî Pioneering study where agents learn compressed generative representations of environments.
- [PlaNet: Learning Latent Dynamics for Planning (Hafner et al., 2019)](https://arxiv.org/abs/1811.04551) ‚Äî Planning via latent dynamics models.
- [Dreamer: Scalable Reinforcement Learning Using World Models (Hafner et al., 2020)](https://arxiv.org/abs/1912.01603) ‚Äî Agents learn from imagined rollouts generated by world models.
- [DreamerV3 (Hafner et al., 2023)](https://arxiv.org/abs/2301.04104) ‚Äî Extends Dreamer for generalization across diverse environments.
- [Understanding World or Predicting Future? A Comprehensive Survey of World Models (2025)](https://arxiv.org/html/2411.14499) ‚Äî Survey contrasting world-model approaches: understanding vs predicting.


### Generative & Interactive Environments
- [Genie: Generative Interactive Environments (Bruce et al., 2024)](https://arxiv.org/abs/2402.15391) ‚Äî Unsupervised interactive environment model learned from videos.
- [Genie-2: Large-Scale Foundation World Model (DeepMind, 2024)](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/) ‚Äî Generates diverse, playable 3D worlds from images.
- [Genie-3: A New Frontier for World Models (DeepMind, 2025)](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/) ‚Äî Real-time interaction with improved realism.
- [Genie Envisioner (arxiv 2025.08)](https://arxiv.org/abs/2508.05635) [Website](https://genie-envisioner.github.io/) ‚Äî Extends Genie to robotic manipulation tasks.
- [RoboGen: Infinite Data for Robot Learning (2023)](https://arxiv.org/abs/2311.01455) ‚Äî Generates large datasets for robot learning via simulation.


### Embodied Agents & Generalist Models
- [SIMA: Generalist AI Agent for 3D Virtual Environments (DeepMind, 2024)](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/) ‚Äî Language-guided agent in multiple 3D environments.
- [Embodied AI Agents: Modeling the World (Fung et al., 2025)](https://arxiv.org/abs/2506.22355) ‚Äî Unified frameworks for embodied world modeling.
- [World Model Implanting for Test-time Adaptation (ICML 2025)](https://arxiv.org/abs/2509.03956) ‚Äî Injects world-model priors into agents for fast adaptation.
- [Learning Primitive Embodied World Models (arxiv 2025.08)](https://arxiv.org/pdf/2508.20840) [Website](https://qiaosun22.github.io/PrimitiveWorld/) ‚Äî Primitive-based world models for scalable learning.
- [FOUNDER (ICML 2025)](https://arxiv.org/abs/2507.12496) [Website](https://sites.google.com/view/founder-rl) ‚Äî Combines foundation and world models for open-ended decision-making.
- [Latent Policy Steering with Embodiment-Agnostic World Models (arxiv 2025.07)](https://arxiv.org/abs/2507.13340) ‚Äî Embodiment-agnostic world-model features guide policy.
- [EmbodieDreamer (arxiv 2025.07)](https://arxiv.org/pdf/2507.05198) [Website](https://embodiedreamer.github.io/) ‚Äî Improves real-to-sim-to-real transfer for policies.

### Evaluation & Stability of World Models
- [Do Vision-Language Models Have Internal World Models? (2025)](https://arxiv.org/abs/2506.21876) ‚Äî Tests VLMs as world models.
- [Adapting Vision-Language Models for Evaluating World Models (2025)](https://arxiv.org/abs/2506.17967) ‚Äî Temporally grounded evaluation of rollouts.
- [Toward Stable World Models (2025)](https://arxiv.org/abs/2503.08122) ‚Äî Addresses instability in diffusion-based world models.
- [WorldEval (arxiv 2025.05)](https://arxiv.org/abs/2505.19017) [Website](https://worldeval.github.io/) ‚Äî Benchmarks robot policies inside learned world models.
- [Evaluating Robot Policies in a World Model (arxiv 2025.06)](https://arxiv.org/abs/2506.00613) [Website](https://world-model-eval.github.io) ‚Äî Compares policy performance in simulated world-model environments.


### Gaussian & Diffusion-Based World Models
- [GWM (ICCV 2025)](https://arxiv.org/abs/2508.17600) [Website](https://gaussian-world-model.github.io/) ‚Äî Gaussian representations improve scaling for manipulation.
- [ManiGaussian++ (arxiv 2025.06)](https://arxiv.org/abs/2506.19842) [Code](https://github.com/April-Yz/ManiGaussian_Bimanual) ‚Äî Extends Gaussian modeling to bimanual tasks.
- [ManiGaussian (arxiv 2024.03)](https://arxiv.org/abs/2403.08321) [Code](https://guanxinglu.github.io/ManiGaussian/) ‚Äî Dynamic Gaussian splatting for multi-task manipulation.
- [Physically Embodied Gaussian Splatting (arxiv 2024.06)](https://arxiv.org/abs/2406.10788) [Website](https://embodied-gaussians.github.io/) ‚Äî Real-time Gaussian splatting for robots.
- [DiWA (CoRL 2025)](https://arxiv.org/abs/2508.03645) [Code](https://diwa.cs.uni-freiburg.de) ‚Äî Diffusion-based policy adaptation with world models.
- [Consistent World Models via Foresight Diffusion (arxiv 2025.05)](https://arxiv.org/abs/2505.16474) ‚Äî Long-term consistency with diffusion.
- [LaDi-WM (arxiv 2025.05)](https://arxiv.org/abs/2505.11528) ‚Äî Latent diffusion for predictive manipulation.



### World Models for Manipulation & Control
- [World4Omni (arxiv 2025.06)](https://arxiv.org/abs/2506.23919) [Website](https://world4omni.github.io/) ‚Äî Transfers image-generation world models to robotics.
- [ParticleFormer (arxiv 2025.06)](https://arxiv.org/abs/2506.23126) [Website](https://particleformer.github.io/) ‚Äî 3D point-cloud world model for multi-material manipulation.
- [GAF (arxiv 2025.06)](http://chaiying1.github.io/GAF.github.io/project_page/) ‚Äî Action-field Gaussian world model for manipulation.
- [Prompting with the Future (RSS 2025)](https://arxiv.org/abs/2506.13761) [Website](https://prompting-with-the-future.github.io/) ‚Äî Open-world MPC with interactive digital twins.
- [3DFlowAction (arxiv 2025.06)](https://arxiv.org/abs/2506.06199) ‚Äî 3D flow for cross-embodiment manipulation.
- [ORV (arxiv 2025.06)](https://arxiv.org/abs/2506.03079) [Code](https://github.com/OrangeSodahub/ORV) [Website](https://orangesodahub.github.io/ORV/) ‚Äî 4D occupancy-centric robot video generation.
- [WoMAP (arxiv 2025.06)](https://arxiv.org/abs/2506.01600) ‚Äî Open-vocabulary object localization.
- [Sparse Imagination (arxiv 2025.06)](https://arxiv.org/abs/2506.01392) ‚Äî Sparse visual world-model planning.
- [Humanoid World Models (arxiv 2025.06)](https://arxiv.org/abs/2506.01182) ‚Äî Foundation models for humanoid robotics.
- [Object-Centric World Model (arxiv 2025.03)](https://arxiv.org/abs/2503.06170) ‚Äî Object-centric representations for language-guided manipulation.
- [PIVOT-R (NeurIPS 2024)](https://arxiv.org/pdf/2410.10394) ‚Äî Primitive-driven waypoint-aware manipulation model.
- [Learning 3D Persistent Embodied World Models (arxiv 2025.05)](https://arxiv.org/abs/2505.05495) ‚Äî Persistent 3D structures for robots.
- [TesserAct (arxiv 2025.04)](https://arxiv.org/abs/2504.20995) [Website](https://tesseractworld.github.io/) ‚Äî 4D temporal-spatial embodied world models.
- [PIN-WM (arxiv 2025.04)](https://arxiv.org/abs/2504.16693) ‚Äî Physics-informed non-prehensile manipulation.
- [ManipDreamer (arxiv 2025.04)](https://arxiv.org/abs/2504.16464) ‚Äî Combines action trees and visual guidance for manipulation.

### Language, Planning & Reasoning with World Models
- [From Word Models to World Models (2023)](https://arxiv.org/abs/2306.12672) ‚Äî Bridges LLMs and probabilistic world models for reasoning.
- [LUMOS (ICRA 2025)](https://arxiv.org/abs/2503.10370) [Website](http://lumos.cs.uni-freiburg.de/) ‚Äî Language-conditioned imitation via world models.
- [Perspective-Shifted Neuro-Symbolic World Models (arxiv 2025.03)](https://arxiv.org/abs/2503.20425) ‚Äî Socially-aware navigation via neuro-symbolic reasoning.
- [World Modeling Makes a Better Planner (arxiv 2025.03)](https://arxiv.org/abs/2503.10480) ‚Äî Dual preference optimization for planning.
- [VisualPredicator (arxiv 2024.10)](https://arxiv.org/abs/2410.23156) ‚Äî Neuro-symbolic predicates for robot planning.
- [MindJourney (arxiv 2025.07)](https://arxiv.org/abs/2507.12508) [Website](https://umass-embodied-agi.github.io/MindJourney) ‚Äî Test-time scaling for spatial reasoning.



### Learning Dynamics, Policies & Transfer with World Models
- [RLVR-World (arxiv 2025.05)](https://arxiv.org/abs/2505.13934) [Website](https://thuml.github.io/RLVR-World/) [Code](https://github.com/thuml/RLVR-World) ‚Äî RL-driven world-model training.
- [FlowDreamer (arxiv 2025.05)](https://arxiv.org/abs/2505.10075) [Website](https://sharinka0715.github.io/FlowDreamer/) ‚Äî RGB-D and flow motion representations.
- [OSVI-WM (arxiv 2025.05)](https://arxiv.org/abs/2505.20425) ‚Äî One-shot visual imitation via world-model trajectories.
- [CoEx (arxiv 2025.07)](https://arxiv.org/abs/2507.22281) ‚Äî Co-evolves exploration and world models.
- [Accelerating Model-Based RL with State-Space World Models (arxiv 2025.02)](https://arxiv.org/abs/2502.20168) ‚Äî Speeds up RL using state-space models.
- [Learning Humanoid Locomotion with World Model Reconstruction (arxiv 2025.02)](https://arxiv.org/abs/2502.16230) ‚Äî Locomotion policies via reconstruction.
- [Strengthening Generative Robot Policies through Predictive World Modeling (arxiv 2025.02)](https://arxiv.org/abs/2502.00622) [Website](https://computationalrobotics.seas.harvard.edu/GPC) ‚Äî Predictive modeling to improve generative policies.
- [RoboHorizon (arxiv 2025.01)](https://arxiv.org/abs/2501.06605) ‚Äî LLM-assisted multi-view world models for long-horizon tasks.
- [Dream to Manipulate (arxiv 2024.12)](https://arxiv.org/abs/2412.14957) [Website](https://leobarcellona.github.io/DreamToManipulate/) ‚Äî Compositional imagination empowers imitation learning.



## üíª Open-Source Repositories
- [Genesis](https://github.com/Genesis-Embodied-AI/Genesis)

- [World Models Experiments (Ha & Schmidhuber)](https://github.com/hardmaru/WorldModelsExperiments)
- [DreamerV3 (reimplementation)](https://github.com/danijar/dreamerv3)
- [Mujoco + World Models Examples](https://github.com/openai/mujoco-py)
- [HunyuanWorld-1.0](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)
- [DIAMOND (DIffusion As a Model Of eNvironment Dreams)](https://github.com/eloialonso/diamond)
- [DriveAGI](https://github.com/OpenDriveLab/DriveAGI)
- [DayDreamer](https://github.com/danijar/daydreamer)
- [Open Driving World Models (OpenDWM)](https://github.com/SenseTime-FVG/OpenDWM)
- [Implementation of the DreamerV2 agent in TensorFlow 2](https://github.com/danijar/dreamerv2)
- [PlaNet Implementation (PyTorch)](https://github.com/Kaixhin/PlaNet)
- [EmbodiedGen](https://github.com/HorizonRobotics/EmbodiedGen)
- [awesome-world-models-for-robots](https://github.com/operator22th/awesome-world-models-for-robots)
- [Awesome-World-Models](https://github.com/PatrickHua/Awesome-World-Models)
- [Generative Wolrd Explorer](https://github.com/GenEx-world/genex)
- [GenieRedux](https://github.com/insait-institute/GenieRedux)



## üõ†Ô∏è Tools or Libraries
- [DeepMind Control Suite](https://github.com/deepmind/dm_control) ‚Äì Google DeepMind's software stack for physics-based simulation and RL environment
- [OpenAI Gym / Gymnasium](https://gymnasium.farama.org/) ‚Äì Widely used RL environment suite.
- [Habitat-Sim](https://aihabitat.org/) ‚Äì Embodied AI and navigation simulator.
- [MuJoCo](https://mujoco.org/) ‚Äì High-performance physics engine for model-based RL.

- [PyBullet](https://pybullet.org/) ‚Äì Physics dataset simulator for robotics and RL.

## üìä Benchmarks and Datasets
- [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560)
- [WorldScore: A Unified Evaluation Benchmark for World Generation](https://worldmodelbench.github.io/static/pdfs/2_WorldScore_A_Unified_Evaluat.pdf)
- [WorldSimBench: Towards Video Generation Models as World Simulators](https://worldmodelbench.github.io/static/pdfs/8_WorldSimBench_Towards_Video_.pdf)
- [Beyond Simulation: Benchmarking World Models for Planning and Causality in Autonomous Driving](https://arxiv.org/abs/2508.01922)

- [WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning](https://arxiv.org/abs/2506.04363)
- [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
- [SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model](https://arxiv.org/abs/2503.13952)
- [EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models](https://arxiv.org/abs/2505.09694)
- [ACT-Bench: Towards Action Controllable World Models for Autonomous Driving](https://arxiv.org/abs/2412.05337)
- [Text2World: Benchmarking Large Language Models for Symbolic World Model Generation](https://arxiv.org/abs/2502.13092)
- [Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models](https://arxiv.org/abs/2311.09064)
- [CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks](https://arxiv.org/abs/2406.13945)
- [AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models](https://arxiv.org/pdf/2408.15511)
- [EVA: An Embodied World Model for Future Video Anticipation](https://arxiv.org/abs/2410.15461)



## üéì Courses, Lectures & Tutorials

- [Generally Capable Agents in Open-Ended Worlds, Jim Fan, NVIDIA Lead of Embodied AI](https://youtu.be/ZSPEyFqAGDc?si=hOz3zsayW73wq4A8)
- [Generalization for Robot Learning In The Wild | Embodied AI Lecture series at AI2](https://youtu.be/jCtx4Xqq8-0?si=fFm6S_wbJs296Vta)
- [Toward Generalizable Embodied AI for Machine Autonomy](https://youtu.be/j8XUHM1ZO60?si=5729e3E0dFpbBp46)
- [CVPR 2023 Embodied AI Workshop](https://youtu.be/yGBnZO7z5h4?si=8eYbkpmrT8SSbBvg)
- [From Robotics to AI NPCs](https://youtu.be/JRyvOCjwfGA?si=xt8xDQY2fhIHnmYH)

- [Towards Long-Horizon Robot Task Learning](https://youtu.be/Wl3PaqUrKEc?si=1eWbBoweLA51BAiY)
- [Adapting Like Humans: Embodied AI Beyond Datasets and Domains by Pier Luigi Dovesi](https://youtu.be/VAKyjaa9vcg?si=tZBGNcDxDBBPKmdz)



## üìù Blogs & Explanations
- [Lil‚ÄôLog: Diffusion Models for Video Generation](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)
- [Rohit Bandaru: World Models](https://rohitbandaru.github.io/blog/World-Models/)

- [DeepMind Blog: Genie 3: A new frontier for world models](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)
- [DeepMind Blog: Genie 2: A large-scale foundation world model](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)
- [Gary Marcus: Generative AI‚Äôs crippling and widespread failure to induce robust models of the world](https://garymarcus.substack.com/p/generative-ais-crippling-and-widespread)
- [Xun Huang: Towards Video World Models](https://www.xunhuang.me/blogs/world_model.html)
- [HunyuanWorld-Voyager: Technical Report](https://3d-models.hunyuan.tencent.com/voyager/voyager_en/assets/HYWorld_Voyager.pdf)
- [Runway: Introducing General World Models](https://runwayml.com/research/introducing-general-world-models)

---

üìå *This page is part of the [Robotics Resource Hub](./README.md). Updated biweekly with new resources.*
